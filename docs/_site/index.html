<!DOCTYPE html>
<html lang="en-us">
  <head>
  <!--link href="http://gmpg.org/xfn/11" rel="profile"-->
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
  <title>Packetsneaker Security &middot; Information Security In A Turbulent World.</title>
  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/blackdoc.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=EB+Garamond">
  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>

  <body>
    
    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Packetsneaker Security
        </a>
      </h1>
      <p class="lead">Information Security In A Turbulent World.</p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item active" href="/">Home</a>

      

      
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/about.html">About</a>
          
        
      
        
      
        
          
        
      
        
          
            <a class="sidebar-nav-item" href="/resume.html">Resume</a>
          
        
      
        
      

      <hr>
      <!-- a class="sidebar-nav-item" href="" --><!-- GitHub project --><!-- /a -->
    </nav>

    <p>&copy; 2023. All rights reserved.</p>
  </div>
</div>

    <!--


-->

    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2023/03/19/research-ideas.html">
        Research Project Idea
      </a>
    </h1>

    <span class="post-date">19 Mar 2023</span>

    <p>Seeing all the news lately regarding the advancements made with GPT-4 really got me excited and thinking back to my senior innovation project in college. I actually got so excited I wrote down my thoughts on a potential research project. I warn you though, this is probably going to be a bit long, so stay with me.</p>

<h2 id="abstract">Abstract</h2>
<p>The use of large language models in machine learning has lead to an explosion of tools attempting to improve the productiveness and potency of cybersecurity operations. Many of these efforts appear to be limited in scope and reliant on proprietary methods and data based on the organization’s previous industry experience. This project aims to alleviate that misunderstanding and tightly focus on a broader set of training data specifically chosen to provide the model with a data set focused on a more expansive background than the current offerings in development. If successful, this project will provide security analysts with reliable, accurate feedback regarding any cybersecurity activity analysis executed by the project.*</p>

<h2 id="history">History</h2>
<p>In 2007, as part of my university graduation provisions, which I’ve <a href="/2012/02/06/college-projects.html">spoken about</a> before, I was required to choose a topic for my capstone. Being a network security student interested in how malicious software worked and having an interest in protecting things as part of my electronic countermeasures field in the military, I chose the topic of proactive computer and network defense. In working through this project from 2007 to 2012; fleshing it out and attempting to build a prototype, I created Project Corezashi. The idea behind Corezashi was very similar to <em>this</em> project in that I proposed using CAPEC data, AV definitions, malware source code, and SANS report data. The difference*, at this point, is that Corezashi was intended to have what I called an countermeasures signature engine that would generate countermeasures based on that data and pass them along to another feature I dubbed the threat analysis engine to verify their effectiveness.</p>

<h2 id="proposal">Proposal</h2>
<p>GPT (Generative Pre-trained Transformer) in its various forms from OpenAI, is what’s called a large language model in the form of a chat bot designed to respond to questions in natural human language with answers in natural human language. As such and according to information that can be found regarding the project, it is a neural network model using supervised and reinforcement learning techniques trained on five distinct data sets. These data sets include the Common Crawl, WebText2, Books1, Books2, and Wikipedia. While these data sets are certainly comprehensive for the most part, I have been unable to verify that they contain the full context of things like MITRE CAPEC and ATT&amp;CK. With the recent release of the GPT-4 model for public interaction and the open sourcing and release of the GPT-3/3.5 code, it may be time to re-evaluate the feasibility of a project similar to Corezashi.</p>

<p>What I am proposing is actually a two-fold project. First, train a GPT style model on the following specific data sets in addition to the five sets mentioned previously.</p>

<ul>
  <li><a href="https://capec.mitre.org">MITRE CAPEC</a></li>
  <li><a href="https://attack.mitre.org">MITRE ATT&amp;CK</a></li>
  <li><a href="https://maecproject.github.io">MITRE MAEC</a></li>
  <li>Threat intelligence reporting (Potentially a project of its own to prepare it as training data)</li>
  <li><a href="https://zeek.org">Zeek</a>/<a href="https://www.corelight.com">Corelight</a> logs</li>
  <li>Source code</li>
</ul>

<p>CAPEC (Common Attack Pattern Enumerations and Classifications) “is a comprehensive dictionary and classification taxonomy of known attacks typically used to advance the community’s understanding and enhance defenses” according to MITRE’s website. The data from CAPEC is based on CVE (Common Vulnerabilities and Exposures) and CWE (Common Weakness Enumeration) datasets. The good news here is that the CAPEC data appears to be available in a format conducive to the required pre-processing for traditional machine learning. This means that there is potentially less work required in order to use it as training data.</p>

<p>ATT&amp;CK (Adversarial Tactics, Techniques and Common Knowledge) is focused on the network defense and describing the operational phases in an adversary’s life cycle with tactics, techniques, and procedures according to MITRE’s website. Also according to MITRE, many of hte attack patterns enumerated by CAPEC are used by adversaries via specific techniques described by ATT&amp;CK which are cross referenced “where appropriate”. This data also appears to be available in a format conducive to pre-processing efforts, making it less of a struggle to use, hopefully.</p>

<p>The MAEC project seems to enjoy less direct attention from MITRE, since it doesn’t have a page on their own site. However, the Malware Attribute Enumeration and Characterization project is intended to be a community-developed structured language for encoding and sharing high-fidelity information about malware based on attributes such as behaviors, artifacts, and relationships between samples according to it’s website. This project has also apparently been incorporated by a few organizations that you may all have heard of, Joe Sandbox, Palo Alto, Cuckoo, Reversing Labs, and Bromium. Many of the web pages for this project appear to be broken and work appears to have paused or stopped in October of 2022.</p>

<p>The collection, correlation, and processing of threat intelligence information regarding malware and adversaries is where MAEC and ATT&amp;CK seem to be stemming from. While it may be easier to use those projects exclusively, due to the lack of recent work on MAEC and the apparent lack of correlation between MAEC/ATT&amp;CK/CAPEC, it may be better to just pull in all of the professional threat intelligence feeds feasible and compile this dataset ourselves. Performing these actions is no trivial matter could be a project in-and-of-itself. This is not a journey to embark upon lightly. It would require the parsing and formatting of all reporting into a standardized dataset including all artifacts/IOCs as well as adversarial identification and correlation where possible.</p>

<p>Working with Zeek or Corelight logs would potentially be less complicated than other datasets due to the fact that they are stored in a standardized manner across multiple types that lends itself to easier integration as a dataset. Benefits of this data type include the ability to use known good and known bad traffic that is easily identified and comes pre-formatted for simple pre-processing as already mentioned. Beyond the initial dataset for training, should this project prove successful, this source of data could be potentially analyzed by a trained model in real-time as a next generation method of indentification and countermeasures to malicious activity.</p>

<p>Using source code where available of known good and known bad software would in theory work the same way the Common Crawl dataset works currently. GPT has shown itself to “understand” programming and code as well as having the ability to provide recommendations regarding the improvement or fixing given code sequences when interacting with an analyst.</p>

<h2 id="previoussimilar-work">Previous/Similar Work</h2>
<p>Prior work in this realm has been performed by many organizations at this point. Some of the earliest examples include Cylance back in late 2012, which ultimately failed if I remember correctly. Current examples include CrowdStrike, Fortinet, Vectra, Nvidia, Sophos/Trellix, CheckPoint, and DarkTrace to name a few. This list of companies allegedly using machine learning in their products is quite large since the idea seems to have been renewed with the release of tools like ChatGPT, Bing’s bot, and Google’s Bard.</p>

<p>Most of these instances appear to be either using just the large language model to provide capabilities similar to ChatGPT or using in-house data via their specific industry tooling mixed with common machine learning to provide accelerated insights for customers into what is happening on their networks. MITRE has a project named <a href="https://atlas.mitre.org">ATLAS</a> which appears to be a knowledge base of adversary tactics, techniques, and case studies for machine learning based on real-world observations, demonstrations from red teams and security groups, as well as the “state of the possible” from academic research. ATLAS is modeled after ATT&amp;CK, but is intended to enable researchers to navigate threats specific to machine learning systems themselves according to their website. As such, it may be a candidate for inclusion into the training data.</p>

<p>With that being said, the ideas put forth here are just that, ideas. I am open to suggestions on how to improve this project, corrections of my assumptions, and constructive feedback. The idea is to actually improve cybersecurity, not present snake oil claims that won’t actually work.</p>

<p>Based on this relatively short and limited amount of research, I think there is enough justification for a research project like I have just proposed that would ultimately more tightly tie these projects together and potentially provide incredible results for defensive cybersecurity work.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2018/10/17/OSDFcon-Presentation.html">
        OSDFcon Presentation
      </a>
    </h1>

    <span class="post-date">17 Oct 2018</span>

    <p>My good friend Andrew Quill and I had the honor to present our talk <em>Farming the Loot Cave: Threat Hunting in Memory with the Volatility Framework and Big Data</em> at OSDFcon today and unveil our technical addon for Splunk that allows you to ingest Volatility 2.x results data. May I present <a href="https://splunkbase.splunk.com/app/3919">TA-Volatility</a>! Here’s the <a href="https://github.com/mutedmouse/ta-volatility">source</a> if you’re more interested in that than the Splunk app.</p>

<p>We created TA-Volatility because memory analysis has become a key way to hunt and track malware and advanced persistent threats (APTs). It’s more important than ever to arm your analysts and investigators with better tools and capabilities for memory analysis.  In our talk we showed how to use this app to get your Volatility outputs into Splunk and create simple, yet effective, dashboards to visualize critical artifacts and rapidly pinpoint threats in memory.</p>

<p>TA-Volatility is also extensible, so you can ingest data from additional Volatility and community plugins if the capability isn’t already included.  Here’s the slide deck we presented if you’re curious.</p>
<div><iframe class="speakerdeck-iframe" frameborder="0" src="//www.osdfcon.org/presentations/2018/Andrew-Quill-Farming-the-Loot-Cave.pdf" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true" style="border: 0px; margin: 0px; padding: 0px; border-radius: 5px; width: 710px; height: 430.37499999999955px; background: transparent;"></iframe></div>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2012/02/06/college-projects.html">
        College Projects
      </a>
    </h1>

    <span class="post-date">06 Feb 2012</span>

    <p>During my time in college there were a few projects I was involved with, some as requirements for graduation, others as attempts to improve things for online students at my school.  I’ll go through and talk a little about each one below.</p>

<h2 id="parallel-universe-wwwpuniverseorg">Parallel Universe (<a href="https://web.archive.org/web/20110923115628/http://www.puniverse.org/">www.puniverse.org</a>)</h2>
<p><img align="left" width="200" height="200" src="/public/images/paralleluniverse.png" />
I actually took over and recreated Parallel Universe from a prior University of Advancing Technology student.  It was intended as a club aimed towards online students, where they could collaborate and socialize with each other and the campus students.  As a project, it was my first attempt at managing a web server, club, and forum all at the same time.  I started off running Parallel Universe on a CentOS 4.5 box where I had to learn a lot about how to manage Linux and all of the other software required in running a website.  This includes Apache, MySQL, and the SimpleMachines forum software.</p>

<p>Learning how to properly set up Apache for a basic website took me about two weeks initially.  However, with each upgrade of the site software, that time shrank by a decent amount of time.   Figuring out and learning how MySQL worked was another task entirely.  I spent many an hour using Google to track down the proper way to set it up for tasks such as this one without messing it up first. The SimpleMachines software was mostly just HTML, which while I could read it, I didn’t always understand what it was doing.</p>

<p>Eventually, I migrated Parallel Universe to CentOS 5.2 and my understanding of how the underpinnings of a server running a website on Linux increased by a couple orders of magnitude. Setting up Apache, MySQL, and SimpleMachines was a bit easier and only takes me a couple hours to get the groundwork laid out with an hour to two more depending on what kind of tweaking I wanted to do to the website.</p>

<h2 id="mint-firix-forensic-dvd">Mint-Firix Forensic DVD</h2>
<p>The Mint-Firix project came about as a result of a discussion I had with another classmate in one of my security related classes.  At that point we were trying to use a copy of the Forensic and Incident Response Environment (F.I.R.E.) live-cd and found it to be a bit clunky and not always working right.  I took a newer and more stable Linux distribution, Linux Mint, and tried to create my own live-cd that included the tools that F.I.R.E. had and a few that were included on the Helix3 live-cds.  For lack of a better idea, I called it Mint-Firix.  After going through the process and eliminating packages we didn’t need and adding the ones we wanted, the live-cd turned out quite well for a first try.</p>

<p>I gave a copy of the live-cd to one of my classmates that was interested in seeing if it worked and he reported that it seemed to boot up and work fine in a virtual machine, but the tools weren’t as intuitive as we wanted them to be.  At this point, Mint-Firix needed some tweaking on some of the packages that didn’t quite integrate as well as I had hoped and it needed an update to the user interface. Ideally it would have been something different than the original Mint interface. Basically it needed a facelift and some testing with the packages I included.</p>

<h2 id="project-corezashi---senior-innovation-project-wwwcorezashicom">Project Corezashi - Senior Innovation Project (<a href="https://web.archive.org/web/20110208051614/http://corezashi.com/">www.corezashi.com</a>)</h2>
<p><img align="left" width="200" height="25" src="/public/images/corezashi-trimmed.jpg" /></p>
<div style="text-align: center">Project Corezashi was intended to be a method for proactively protecting computing environments from malware in real-time utilizing neural networks</div>
<p>and artificial intelligence.  The basic thought behind the project was to analyze all of the data we have available to us regarding malware in general and each piece specifically with neural networks looking for patterns that can be acted upon. Once those patterns had been identified, artificial intelligence equivalent to a modified expert system would have been used to automatically generate countermeasures.  After such countermeasures had been identified, they would have been fed back through the neural networks as part of the data for verification and improvement.</p>

<p>I went through a few different iterations on the base idea for this project trying to figure out which language to write it, python or go.  Ultimately, I decided to use python, but I personally didn’t have the required knowledge or skill in python at that point to do much more than a couple token files for downloading some information from ExploitDB and attempting to normalize the data.</p>

<p>UPDATE 3/15/2023 - This idea is still a bit of pet project that I’d love to make some progress on, especially with the latest advancements like GPT4.</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2012/01/07/certifications.html">
        Thoughts on IT Certifcations
      </a>
    </h1>

    <span class="post-date">07 Jan 2012</span>

    <div class="message">
  This is one of my earliest blog entries during the time I was still completing my degree. It was one of my first attempts at analyzing the situation I found myself in.
</div>

<p>I got to thinking yesterday after taking the test for Security+ certification, of what use are IT certifications to the average person hoping to land a job in the IT world?  For those of us looking for a new job position, it’s an almost universal requirement to have some sort of IT certification no matter where you apply for a job at.  Now, if you are an entry level person like me, then they may be a way to prove you know some basics about a subject and are worth hiring.  However, what about those individuals who have the multiple years of experience in a specialized sector of Information Technology, do these individuals really need that certification or should their level of experience speak for itself?</p>

<p>Let’s start by looking at some of the “entry-level” certifications like those from CompTIA.  Most people would agree that A+ and Network+ are your rock bottom entry level certifications.  What does it take to become A+ or Network + certified?</p>

<p>If you are looking to acquire your A+ certification, it is generally recommended that you have roughly 500 hours of hands on experience with basic computer repair, networking, and troubleshooting.  Before taking the Network+, CompTIA recommends achieving A+ certification and having a minimum of 9 months networking experience, but these are not required.  So, assuming you have all this experience, is it really worth your time to get certified?</p>

<p>According to the <a href="https://web.archive.org/web/20090520164226/http://www.footepartners.com/FooteNewsrelease_2009skillstrends_041609V1.pdf">Foote Research Group</a> as of the first quarter of 2009, certifications are becoming increasingly valuable in providing a little more job security in the IT field.  Of specific interest were the security related certifications.  The highest paying certifications tended to be the higher level ones like Microsoft Certified Solution Developer (MCSD) and the Certified Information Security Manager (CISM). There are some skills and certifications of course that have decreased in value, among them are skills in C++ and Network+ certification.</p>

<p>It appears that the question of whether or not you really need to be certified can only be answered in the context of what you hope to be doing in your IT career.  Some workplaces may require you to have certain certifications while others really don’t seem to care a whole lot.  If we were going off of the information presented here, I would say that it would be in everyone’s best interest to acquire at least a couple certifications if you hope to go anywhere in the IT field.  The trick is guessing which ones are going to pay off the most in the long run.</p>

<p>For those people with years of experience it looks like companies still want the appropriate level of certifications from you.  Being certified appears to bring a much higher confidence value in regards to your skills as they apply to your specialization within Information Technology.  This isn’t to say that those people without the certification couldn’t do the job, it just appears that those who are certified have a better chance of keeping their job in the current economy.</p>

<p>Sound off!  Tell me what you think about certifications.</p>

  </div>
  
</div>

<div class="pagination">
  
    <span class="pagination-item older">Older</span>
  
  
    <span class="pagination-item newer">Newer</span>
  
</div>
    </div>
  </body>
</html>
